{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6zZiaD1yJ2TlLvcvyn9Aa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarkar-sayan/URL-Classification/blob/main/POC_1_URL_Classification_User_Interface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdedyzV66MhL",
        "outputId": "d323b670-a1fd-476a-8a24-f384c892815e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code as a user-interface"
      ],
      "metadata": {
        "id": "RR-6jaOYbLVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langdetect googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "id": "bS9ilsNObSlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cd2de6-a8dc-4ce2-f845-19af9f4cc7f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.6.2)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from random import choice\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "# Initialize Google Translator\n",
        "translator = Translator()\n",
        "# Initialize Tokenizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Download the stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  # List of user-agents for rotation\n",
        "user_agents = [\n",
        "     'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "     'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
        "     'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',]\n",
        "\n",
        "global productive_keywords\n",
        "global non_productive_keywords\n",
        "\n",
        "productive_keywords = ['study', 'research', 'education', 'work', 'project', 'python']\n",
        "non_productive_keywords = ['game', 'social', 'fun', 'entertainment', 'video']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQAIFy0FPvOA",
        "outputId": "2a1cd6f3-dfbd-4474-d5c0-a293f9f4cb33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter or delete pre-saved Keywords\n",
        "\n",
        "def keyword_management(productive_keywords, non_productive_keywords):\n",
        "  choice = int(input(\"Enter choice to manage keywords (1 or 2) : 1. Productive  2. Non-productive  3. Display All -> \"))\n",
        "  if(choice == 1):\n",
        "    get_productive(productive_keywords)\n",
        "  elif(choice == 2):\n",
        "    get_nonproductive(non_productive_keywords)\n",
        "  elif choice not in [1, 2]:\n",
        "    print(\"Displaying All....\")\n",
        "    print(\"All Productive Keywords:\", \", \".join(productive_keywords))\n",
        "    print(\"All Non-productive Keywords:\", \", \".join(non_productive_keywords))\n",
        "  return productive_keywords, non_productive_keywords\n",
        "\n",
        "\n",
        "def get_productive(productive_keywords):\n",
        "  ch = int(input(\"Enter choice to manage productive keywords (1 or 2) : 1. Add new  2. Delete old  3. Display all->\"))\n",
        "  if(ch == 1):\n",
        "      c = int(input(\"Add NEW -> Operation mode: 1. Single  2. Bulk : \"))\n",
        "      if(c == 1):\n",
        "        key = input(\"Enter new Productive Keyword: \")\n",
        "        productive_keywords.append(key)\n",
        "      if(c == 2):\n",
        "        while True:\n",
        "          key = input(\"\\nEnter keyword to add (or 'done' to finish): \")\n",
        "          if key.lower() == \"done\":\n",
        "            break\n",
        "          productive_keywords.append(key)\n",
        "  elif(ch == 2):\n",
        "      c = int(input(\"Delete OLD -> Operation mode: 1. Single  2. Bulk : \"))\n",
        "      if(c == 1):\n",
        "        key = input(\"Enter Productive Keyword to delete: \")\n",
        "        productive_keywords.remove(key)\n",
        "      if(c == 2):\n",
        "        while True:\n",
        "          print(f\"\\nCurrent List: {productive_keywords}\")\n",
        "          key = input(\"Enter keyword to remove (or 'done' to finish): \")\n",
        "          if key.lower() == \"done\":\n",
        "            break\n",
        "          try:\n",
        "            productive_keywords.remove(key)\n",
        "          except ValueError:\n",
        "            print(f\"'{key}' not found in the list. Try again.\")\n",
        "        print(\"\\nFinal List: \", productive_keywords)\n",
        "  elif(ch == 3):\n",
        "      print(\"All Productive Keywords: \", \", \".join(productive_keywords))\n",
        "\n",
        "\n",
        "def get_nonproductive(non_productive_keywords):\n",
        "  ch = int(input(\"Enter choice to manage non-productive keywords (1 or 2) : 1. Add new  2. Delete old  3. Display all->\"))\n",
        "  if(ch == 1):\n",
        "      c = int(input(\"Add NEW -> Operation mode: 1. Single  2. Bulk : \"))\n",
        "      if(c == 1):\n",
        "        key = input(\"Enter new Non-productive Keyword: \")\n",
        "        non_productive_keywords.append(key)\n",
        "      if(c == 2):\n",
        "        while True:\n",
        "          key = input(\"\\nEnter keyword to add (or 'done' to finish): \")\n",
        "          if key.lower() == \"done\":\n",
        "            break\n",
        "          non_productive_keywords.append(key)\n",
        "  elif(ch == 2):\n",
        "      c = int(input(\"Delete OLD -> Operation mode: 1. Single  2. Bulk : \"))\n",
        "      if(c == 1):\n",
        "        key = input(\"Enter Non-productive Keyword to delete: \")\n",
        "        non_productive_keywords.remove(key)\n",
        "      if(c == 2):\n",
        "        while True:\n",
        "          print(f\"\\nCurrent List: {non_productive_keywords}\")\n",
        "          key = input(\"Enter keyword to remove (or 'done' to finish): \")\n",
        "          if key.lower() == \"done\":\n",
        "            break\n",
        "          try:\n",
        "            non_productive_keywords.remove(key)\n",
        "          except ValueError:\n",
        "            print(f\"'{key}' not found in the list. Try again.\")\n",
        "        print(\"\\nFinal List:\", non_productive_keywords)\n",
        "  elif(ch == 3):\n",
        "      print(\"All Non-productive Keywords: \", \", \".join(non_productive_keywords))\n",
        "\n"
      ],
      "metadata": {
        "id": "kbmy2FiGqLsg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get metadata from URL\n",
        "def get_metadata_from_url(url):\n",
        "    try:\n",
        "        headers = {'User-Agent': choice(user_agents)}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        title = soup.find('title').get_text() if soup.find('title') else 'No title'\n",
        "\n",
        "        description = soup.find('meta', attrs={'name': 'description'})\n",
        "        if description:\n",
        "            description = description.get('content')\n",
        "        else:\n",
        "            description = soup.find('meta', attrs={'property': 'og:description'})\n",
        "            description = description.get('content') if description else 'No description'\n",
        "\n",
        "        image = soup.find('meta', attrs={'property': 'og:image'})\n",
        "        if image:\n",
        "            image = image.get('content')\n",
        "        else:\n",
        "            image = soup.find('link', attrs={'rel': 'image_src'})\n",
        "            image = image.get('href') if image else 'No image'\n",
        "\n",
        "        text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "            \"image\": image,\n",
        "            \"url\": url,\n",
        "            \"text\": text\n",
        "        }\n",
        "    except requests.RequestException as e:\n",
        "        return {\n",
        "            \"title\": \"Error\",\n",
        "            \"description\": str(e),\n",
        "            \"image\": \"No image\",\n",
        "            \"url\": url,\n",
        "            \"text\": \"\"\n",
        "        }\n",
        "\n",
        "# Preprocess text function\n",
        "def preprocess_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Translate text if not in English\n",
        "def translate_text_if_needed(text):\n",
        "    try:\n",
        "        language = detect(text)\n",
        "        if language != 'en':\n",
        "            translated = translator.translate(text, dest='en')\n",
        "            return translated.text\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return text\n",
        "\n",
        "# Preprocess metadata function\n",
        "def preprocess_metadata(metadata):\n",
        "    title = translate_text_if_needed(metadata.get('title', ''))\n",
        "    description = translate_text_if_needed(metadata.get('description', ''))\n",
        "    text = translate_text_if_needed(metadata.get('text', ''))\n",
        "    clean_title = preprocess_text(title)\n",
        "    clean_description = preprocess_text(description)\n",
        "    clean_text = preprocess_text(text)\n",
        "    combined_clean_content = f\"{clean_title} {clean_description} {clean_text}\".strip()\n",
        "    return combined_clean_content\n",
        "\n",
        "# Function to extract domain from URL\n",
        "def extract_domain(url):\n",
        "    return url.split('//')[-1].split('/')[0]\n",
        "\n",
        "# Keyword matching function\n",
        "def count_keywords(text, keywords):\n",
        "    tokens = text.split()\n",
        "    return sum(token in keywords for token in tokens)\n",
        "\n",
        "# Function to create feature matrix\n",
        "def create_feature_matrix(df):\n",
        "    tfidf_matrix = vectorizer.fit_transform(df['clean_content'])\n",
        "\n",
        "    keyword_counts = df[['productive_keyword_count', 'non_productive_keyword_count']].values\n",
        "    return np.hstack((tfidf_matrix.toarray(), keyword_counts))"
      ],
      "metadata": {
        "id": "2FgEXFEYcPls"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(productive_keywords, non_productive_keywords, dataset_url):\n",
        "  data = pd.read_csv(dataset_url)\n",
        "  data.drop(data.columns.difference(['url', 'label']), axis=1, inplace=True)\n",
        "  # Scrape Content and Metadata\n",
        "  data['metadata'] = data['url'].apply(get_metadata_from_url)\n",
        "  # Apply Preprocessing to Metadata\n",
        "  data['clean_content'] = data['metadata'].apply(preprocess_metadata)\n",
        "  data['domain'] = data['url'].apply(extract_domain)\n",
        "  data['productive_keyword_count'] = data['clean_content'].apply(lambda x: count_keywords(x, productive_keywords))\n",
        "  data['non_productive_keyword_count'] = data['clean_content'].apply(lambda x: count_keywords(x, non_productive_keywords))\n",
        "  return data"
      ],
      "metadata": {
        "id": "InJ1KuVRcg6F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_model(data):\n",
        "  X = create_feature_matrix(data)\n",
        "  y = data['label']\n",
        "\n",
        "  # Train/Test Split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, shuffle=True)\n",
        "\n",
        "  # Train Model\n",
        "  model = MultinomialNB()\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Evaluate Model\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "  #print(classification_report(y_test, y_pred))\n",
        "  return model"
      ],
      "metadata": {
        "id": "qx6SC5UgpW9C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_url(productive_keywords, non_productive_keywords, model, url):\n",
        "    metadata = get_metadata_from_url(url)\n",
        "    clean_content = preprocess_metadata(metadata)\n",
        "    vectorized_content = vectorizer.transform([clean_content])\n",
        "    keyword_counts = np.array([[\n",
        "        count_keywords(clean_content, productive_keywords),\n",
        "        count_keywords(clean_content, non_productive_keywords)\n",
        "    ]])\n",
        "    feature_vector = np.hstack((vectorized_content.toarray(), keyword_counts))\n",
        "    return model.predict(feature_vector)[0]\n"
      ],
      "metadata": {
        "id": "NiSBTQyFqBAT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_execute(productive_keywords, non_productive_keywords):\n",
        "  print(\"All Productive Keywords:\", \", \".join(productive_keywords))\n",
        "  print(\"All Non-productive Keywords:\", \", \".join(non_productive_keywords))\n",
        "\n",
        "  while True:\n",
        "    option = input(\"Do you want to modify? Y or N : \")\n",
        "    if ((option == 'Y') or (option == 'y')):\n",
        "      productive_keywords, non_productive_keywords = keyword_management(productive_keywords, non_productive_keywords)\n",
        "    else:\n",
        "        print(\"All Productive Keywords:\", \", \".join(productive_keywords))\n",
        "        print(\"All Non-productive Keywords:\", \", \".join(non_productive_keywords))\n",
        "        break\n",
        "\n",
        "  prod = productive_keywords\n",
        "  non_prod = non_productive_keywords\n",
        "\n",
        "  data = prepare_dataset(prod, non_prod, '/content/drive/MyDrive/Sayan RP files/Datasets/URL_Dataset(Sheet1).csv')\n",
        "  model = train_test_model(data)\n",
        "\n",
        "  new_url = input(\"Enter url to check for Productive or Non-productive: \")\n",
        "  print(f\"The URL {new_url} is classified as {classify_url(prod, non_prod, model, new_url)}\")"
      ],
      "metadata": {
        "id": "gVrjuyUf5pyr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  final_execute(productive_keywords, non_productive_keywords)"
      ],
      "metadata": {
        "id": "KWLOYgfKcfxj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL_21oeGSlKb",
        "outputId": "bc92668d-1bd3-4102-a71a-a3de65905a92"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Productive Keywords: study, research, education, work, project, python\n",
            "All Non-productive Keywords: game, social, fun, entertainment, video\n",
            "Do you want to modify? Y or N : y\n",
            "Enter choice to manage keywords (1 or 2) : 1. Productive  2. Non-productive  3. Display All -> 2\n",
            "Enter choice to manage non-productive keywords (1 or 2) : 1. Add new  2. Delete old  3. Display all->1\n",
            "Add NEW -> Operation mode: 1. Single  2. Bulk : 1\n",
            "Enter new Non-productive Keyword: reels\n",
            "Do you want to modify? Y or N : n\n",
            "All Productive Keywords: study, research, education, work, project, python\n",
            "All Non-productive Keywords: game, social, fun, entertainment, video, reels\n",
            "Accuracy: 1.0\n",
            "Enter url to check for Productive or Non-productive: https://about.instagram.com/blog/announcements/introducing-instagram-reels-announcement\n",
            "The URL https://about.instagram.com/blog/announcements/introducing-instagram-reels-announcement is classified as Non-productive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyKHsdASdskp",
        "outputId": "d8e2acdc-807a-4329-d8a9-7d3d5770c6bf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Productive Keywords: study, research, education, work, project, python\n",
            "All Non-productive Keywords: game, social, fun, entertainment, video, reels\n",
            "Do you want to modify? Y or N : n\n",
            "All Productive Keywords: study, research, education, work, project, python\n",
            "All Non-productive Keywords: game, social, fun, entertainment, video, reels\n",
            "Accuracy: 1.0\n",
            "Enter url to check for Productive or Non-productive: https://www.geeksforgeeks.org/python-do-while/\n",
            "The URL https://www.geeksforgeeks.org/python-do-while/ is classified as Productive\n"
          ]
        }
      ]
    }
  ]
}